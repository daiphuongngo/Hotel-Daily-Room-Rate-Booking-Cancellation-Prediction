---
title: "Hotel-Daily-Room-Rate-Booking-Cancellation-Prediction"
output: pdf_document
date: "2025-04-14"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown


Feature Preparation
Cast reservation_status variable’s categorical values to numerical values → delete this variable
The reservation_status variable has 3 distinct values: Check Out, Canceled, No Shows. It has a strong correlation with the is_canceled variable. It is not meaningful in terms of predicting room rate or cancellation chance as in the real business, this attribute is unknown when given with new sets of booking attributes.

The data was identified with 4 rows with missing values in the children variable so they can be considered as no children checking in the hotel. So these null values are casted as 0 to illustrate no children stayed with parents in the booking.

Convert Date columns to datetime format → predefined date of week, month of year
The date-type features of Booking Date, Check In Date, Check Out Date are also calculated to be used for trend analysis and Time Series Forecasting. However, for the Regression and Classification parts, they are not used.

Retain only rows where ADR is not equal to 0 
Each row represents one booking. If the booking has 0 value in the ADR, they are not accommodation bookings but complimentary accommodation bookings in case that the number of stays is positive. In contrast, they are not complimentary accommodation bookings but meal service booking only at restaurants, lounges, bars, etc.
Reassign room type features
In the reserved_room_type and assigned_room_type features, there are some room types in the minority classes that can be considered as top luxury levels. However, the volume of them is not high enough so they could potentially cause skewed distribution in the ADR target feature. Therefore, those room types with low frequency and top luxury levels will be grouped into “Other” to represent the most luxurious room types in order to not affect the distribution of target variables when splitting the data by different ratios in the next step.

Convert categorical columns to factors in R
This step is executed after all data transformations on categorical variables are accomplished as categorical variables should be handled when their original value’s types are character.
Check for factors with only one level
Another step is added to check if any factorized categorical variables have only one single value that might cause unknown values for models to learn from.
Removed company, agent, country → transform country
From the hotel business sense, the room rate planning, strategy and tactic are assigned to different Sales segments, including: corporate, leisure, group with accommodation and events, online and offline travel agency, hotel website, etc. Therefore, the company nature of clients and booking inquiries is important in senior management’s decision making as they might involve specific relationships and other secret deals and factors between the hotel and their corporate clients or firms. It is the same case with travel agencies. Without further attributes and specific knowledge about the real corporate firms’ names that were hidden. There are too many names and scenarios that will affect the model performance for the two goals in this paper. Therefore, these two features will be eliminated due to the scope of this analysis in testing the hypotheses and evaluating different models’ performance.

Min max scale for numerical features was also applied as the data has skewed distribution in most of the numerical features. This technique is used to normalize the data from numerical features to train the models without skewness. The models’ performance will be evaluated to find if the normalization by this technique could help improve the metrics  across splitting types.

Plot correlation matrix
Looking at the correlation plot, we should focus on the dark blue dots in the middle of the triangle plot. There are strong correlations between stays_in_weekend_nights, stays_in_weekday_nights with the Length of Stay LoS and Average Room Revenue per Booking. As explained in the variable information section, the LoS is a sum of both stays in weekend and weekday nights. Indeed, it should be a standalone feature to represent the total room nights per booking whether the nights are in the weekday or weekend or both. The Average Room Revenue per Booking is calculated by summing up all room revenue by each day and dividing by the number of bookings by corresponding date. Therefore, they should be treated as an independent variable from the other two variables despite strong correlations between them.

Significance level

Regarding the Regression part, 
Thanks to the Linear Regression on all dataset using specified features as predictors, all newly calculated features are statistically significant in predicting the ADR. Except the one variable of LoS that caused NA in the coefficients.
Thanks to the Decision on all dataset using specified features as predictors, all newly calculated features, including the LoS and excluding Booking Date, Check In and Check Out Dates are statistically significant in predicting the ADR. Hence, we can reject the null hypothesis.

Next, the VIF and StepAIC technique with both directions was applied to the base Linear Regression Model to find the best set of most important features in predicting the ADR and the optimal model at the end can provide this set to the next step while showing no more NA on the Length of Stay LoS. Further analyses using the Linear Regression and Decision Tree models on the same filtered dataset with the given set of variables thanks to the optimal model showing with different splitting types as reported in the below table:

Metrics

Regarding the Regression models, three metrics are used to determine the performance of them, including Mean Squared Error (MSE), Root Mean Square Error (RMSE), Mean Absolute Error (MAE). 
Regarding the Classification models, four metrics are used to determine the performance of them, including Accuracy, Sensitivity (Recall), Specificity, Pos Pred Value (Precision).

Model Performance Comparison:

Looking at the Linear Regression table without normalization, although it was configured with stratified balance in the splitting codes, the best model in terms of all three metrics is the one with the splitting ratio 80/20 while the worst performer is the one with the splitting ratio 70/30.

Meanwhile, the Decision Tree also shows the best model in terms of metrics, except MAE, is the one with the splitting ratio 80/20 while the worst performer is the one with the splitting ratio 70/30 in terms of all three metrics. The MAE of the model having splitting ratio 80/20 is slightly higher than the model having the splitting ratio 50/50 by 0.6.

When comparing the 2 models in terms of the three metrics, overall, the Linear Regression performed way better than the Decision, except the cases with splitting type 70/30 on the testing set in terms of MSE and RMSE. After taking advantage of the Min Max Scaler on numerical features, the best performer in testing data is still the model with 80/20 split while the worst is model with 70/30 split. And in general, the Linear Regression models performed better than the Decision Tree.
Likewise, regarding the classification, the Logistic Regression model with 80/20 split performed the best at accuracy and sensitivity while its specificity and precision are the lowest. The lowest accuracy and recall can be seen in the model with 50/50 split while the highest specificity and precision can be seen in the model with 70/30 split. 
The Decision Tree model with 80/20 saw the highest values in accuracy and sensitivity but the lowest values in specificity and precision. In contrast, the Decision Tree model with 50/50 split experienced the other way around.

—> produce normality assumption on linear reg: errors between pred - actual and build histogram if it is bell shaped


In conclusion, all newly calculated fields are considered statistically significant. So we can reject the null hypothesis #3.

In the Classification part, the VIF and StepAIC were not executed as the R code generated a warning. Therefore, after splitting the data by different ratios with stratified balance on the target variable “is_canceled”, the models of Logistic Regression and Decision Tree use most of the predictors from the dataset that comes from the original and was filtered with some manipulation steps. 


Codes:
VIF:
```{r}
base.model <- lm(adr ~ ., data = hotel.df)

summary(base.model)
```

```{r}
vif(base.model)
```

```{r}
optimal.model = stepAIC(base.model, direction = "both")

# Summary of the selected model
summary(optimal.model)
```

After running Linear Reg:
```{r}
# Predict the response variable (adr) using the linear model and train data
train_predictions <- predict(m, newdata = train)

# Calculate the residuals
train_residuals <- train$adr - train_predictions

# Calculate mean squared error (MSE) for train set
train_mse <- mean(train_residuals^2)

# Calculate root mean squared error (RMSE) for train set
train_rmse <- sqrt(train_mse)

# Calculate mean absolute error (MAE) for train set
train_mae <- mean(abs(train_residuals))

# Print performance measurements for train set
cat("Performance Measurements on Train Set:\n")
cat("Mean Squared Error (MSE):", train_mse, "\n")
cat("Root Mean Squared Error (RMSE):", train_rmse, "\n")
cat("Mean Absolute Error (MAE):", train_mae, "\n")
```

```{r}
# Predict the response variable (adr) using the linear model and test data
test_predictions <- predict(m, newdata = test)

# Calculate the residuals
residuals <- test$adr - test_predictions

# Calculate mean squared error (MSE)
mse <- mean(residuals^2)

# Calculate root mean squared error (RMSE)
rmse <- sqrt(mse)

# Calculate mean absolute error (MAE)
mae <- mean(abs(residuals))

# Print performance measurements
cat("Performance Measurements on Test Set:\n")
cat("Mean Squared Error (MSE):", mse, "\n")
cat("Root Mean Squared Error (RMSE):", rmse, "\n")
cat("Mean Absolute Error (MAE):", mae, "\n")
```

After running Decision Tree (regression) model:
```{r}
# Predict the response variable (adr) using the DT model and train data
train_predictions <- predict(tree_model, newdata = train)

# Calculate the residuals
train_residuals <- train$adr - train_predictions

# Calculate mean squared error (MSE) for train set
train_mse <- mean(train_residuals^2)

# Calculate root mean squared error (RMSE) for train set
train_rmse <- sqrt(train_mse)

# Calculate mean absolute error (MAE) for train set
train_mae <- mean(abs(train_residuals))

# Print performance measurements for train set
cat("Performance Measurements on Train Set:\n")
cat("Mean Squared Error (MSE):", train_mse, "\n")
cat("Root Mean Squared Error (RMSE):", train_rmse, "\n")
cat("Mean Absolute Error (MAE):", train_mae, "\n")
```

```{r}
# Predict the response variable (adr) using the DT model and test data
test_predictions <- predict(tree_model, newdata = test)

# Calculate the residuals
residuals <- test$adr - test_predictions

# Calculate mean squared error (MSE)
mse <- mean(residuals^2)

# Calculate root mean squared error (RMSE)
rmse <- sqrt(mse)

# Calculate mean absolute error (MAE)
mae <- mean(abs(residuals))

# Print performance measurements
cat("Performance Measurements on Test Set:\n")
cat("Mean Squared Error (MSE):", mse, "\n")
cat("Root Mean Squared Error (RMSE):", rmse, "\n")
cat("Mean Absolute Error (MAE):", mae, "\n")
```

After running Decision Tree (classification) model:
```{r}
rpart.plot(final_model)
printcp(final_model)
plotcp(final_model)
```

```{r}
# Predict class labels for training data
train_predictions <- predict(final_model, newdata = train)
train_predictions <- as.factor(train_predictions)

# Predict class labels for testing data
test_predictions <- predict(final_model, newdata = test)
test_predictions <- as.factor(test_predictions)

# Calculate confusion matrices for both training and testing data
confusion_train <- table(predictions = train_predictions, reference = as.factor(train$is_canceled))
confusion_test <- table(predictions = test_predictions, reference = as.factor(test$is_canceled))

# Calculate misclassification error for both training and testing data
error_train <- 1 - sum(diag(confusion_train)) / sum(confusion_train)
error_test <- 1 - sum(diag(confusion_test)) / sum(confusion_test)

# Output results
cat("Final Decision Tree Model (with only significant variables):\n\n")
cat("Confusion Matrix for Training Data:\n")
print(confusion_train)
cat("\nConfusion Matrix for Testing Data:\n")
print(confusion_test)
cat("\nMisclassification Error for Training Data:", error_train, "\n")
cat("Misclassification Error for Testing Data:", error_test, "\n")
```
```{r}
# Load the required library
library(caret)
# Define the predictions for the training and testing sets
# Predict class labels for training data
train_predictions <- predict(final_model, newdata = train)
train_predictions <- as.factor(as.numeric(train_predictions > 0.5))

# Predict class labels for testing data
test_predictions <- predict(final_model, newdata = test)
#test_predictions <- as.factor(test_predictions)
test_predictions <- as.factor(as.numeric(test_predictions > 0.5))

# Compute confusion matrices
train_cm <- confusionMatrix(train_predictions, reference = as.factor(train$is_canceled), positive = '1')
test_cm <- confusionMatrix(test_predictions, reference = as.factor(test$is_canceled), positive = '1')
# Plot confusion matrix for training data
train_cm
# Plot confusion matrix for testing data
test_cm
```

```{r}
# Predict class labels for training data
train_predictions <- predict(final_model, newdata = train)
#pvals_train <- as.factor(train_predictions)

# Predict class labels for testing data
test_predictions <- predict(final_model, newdata = test)
#pvals_test <- as.factor(test_predictions)

# Ensure train_predictions and test_predictions are numeric
train_predictions <- as.numeric(train_predictions > 0.5)
test_predictions <- as.numeric(test_predictions > 0.5)

# Compute ROC curve and AUC for train and test sets
train_roc <- roc(train$is_canceled, train_predictions)
test_roc <- roc(test$is_canceled, test_predictions)
```

```{r}
# Plot ROC curve for train set
plot(train_roc, col = 'red', print.auc = TRUE, auc.polygon = TRUE,
auc.polygon.col = 'lightblue', grid = c(0.1, 0.2),
grid.col = c('green', 'red'), print.thres = TRUE,
main = 'AUC - ROC Curve (Training)')
```

```{r}
# Plot ROC curve for test set
plot(test_roc, col = 'blue', print.auc = TRUE, auc.polygon = TRUE,
auc.polygon.col = 'lightblue', grid = c(0.1, 0.2),
grid.col = c('green', 'red'), print.thres = TRUE,
main = 'AUC - ROC Curve (Testing)')
```

After running Logistic Regression (classification) model:
```{r}
# Predict probabilities and classify specimens
pvals <- predict(final_model, type = "response", newdata = train)
train_predictions <- as.factor(as.numeric(pvals > 0.5))
pvals_test <- predict(final_model, type = "response", newdata = test)
test_predictions <- as.factor(as.numeric(pvals_test > 0.5))
# Calculate confusion matrices for both training and testing data
confusion_train <- table(predictions = train_predictions, reference = as.factor(train$is_canceled))
confusion_test <- table(predictions = test_predictions, reference = as.factor(test$is_canceled))
# Calculate misclassification error for both training and testing data
error_train <- 1 - sum(diag(confusion_train)) / sum(confusion_train)
error_test <- 1 - sum(diag(confusion_test)) / sum(confusion_test)
# Output results
cat("Final Logistic Regression Model (with only significant variables):\n")
cat("\n")
cat("Confusion Matrix for Training Data:\n")
print(confusion_train)
cat("\n")
cat("Confusion Matrix for Testing Data:\n")
print(confusion_test)
cat("\n")
cat("Misclassification Error for Training Data:", error_train, "\n")
cat("Misclassification Error for Testing Data:", error_test, "\n")
```

```{r}
# Load the required library
library(caret)
# Define the predictions for the training and testing sets
train_predictions <- train_predictions
test_predictions <- predict(final_model, newdata = test, type = "response")
test_predictions <- as.factor(as.numeric(test_predictions > 0.5))

# Compute confusion matrices
train_cm <- confusionMatrix(train_predictions, reference = as.factor(train$is_canceled), positive = '1')
test_cm <- confusionMatrix(test_predictions, reference = as.factor(test$is_canceled), positive = '1')
# Plot confusion matrix for training data
train_cm
# Plot confusion matrix for testing data
test_cm
```
```{r}
# Predict probabilities and classify specimens
pvals <- predict(final_model, type = "response", newdata = train)
train_predictions <- as.factor(as.numeric(pvals > 0.5))
pvals_test <- predict(final_model, type = "response", newdata = test)
test_predictions <- as.factor(as.numeric(pvals_test > 0.5))
# Ensure train_predictions and test_predictions are numeric
train_predictions <- as.numeric(train_predictions)
test_predictions <- as.numeric(test_predictions)
# Compute ROC curve and AUC for train and test sets
train_roc <- roc(train$is_canceled, pvals)
test_roc <- roc(test$is_canceled, pvals_test)
```

```{r}
# Plot ROC curve for train set
plot(train_roc, col = 'red', print.auc = TRUE, auc.polygon = TRUE,
auc.polygon.col = 'lightblue', grid = c(0.1, 0.2),
grid.col = c('green', 'red'), print.thres = TRUE,
main = 'AUC - ROC Curve (Training)')
```

```{r}
# Plot ROC curve for test set
plot(test_roc, col = 'blue', print.auc = TRUE, auc.polygon = TRUE,
auc.polygon.col = 'lightblue', grid = c(0.1, 0.2),
grid.col = c('green', 'red'), print.thres = TRUE,
main = 'AUC - ROC Curve (Testing)')
```
=================================================




=============================

## Dates
# arrival and entry date
data <- data %>%
  mutate(
    arrival_date = dmy(paste(arrival_date_day_of_month, 
                             arrival_date_month, 
                             arrival_date_year, 
                             sep = "-")),
    entry_date = arrival_date - days(lead_time)
  )

# data$reservation_status_date <- as.Date(data$reservation_status_date, format = "%Y-%m-%d")

#
data$Check.In.Date <- as.Date(data$Check.In.Date, format = "%m/%d/%y")
data$Check.Out.Date <- as.Date(data$Check.In.Date, format = "%m/%d/%y")

data$Booking.Date <- as.Date(data$Booking.Date, format = "%m/%d/%y")

data_names <- c("arrival_date", "entry_date", "Check.In.Date", "Check.Out.Date", "Booking.Date")
##





















## Order by date
data <- data[order(data$entry_date, decreasing = FALSE), ]
##


# Convert to integer
if (is.character(data$arrival_date_month)) {
  data$arrival_date_month <- match(data$arrival_date_month, month.name)
}

week.name <- c("Sunday", "Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday")

# Convert to integer
if (is.character(data$Booking.Date...day.of.week)) {
  data$Booking.Date...day.of.week <- match(data$Booking.Date...day.of.week, week.name)
}

if (is.character(data$Check.In.Date...day.of.the.week)) {
  data$Check.In.Date...day.of.the.week <- match(data$Check.In.Date...day.of.the.week, week.name)
}

if (is.character(data$Check.Out.Date...day.of.week)) {
  data$Check.Out.Date...day.of.week <- match(data$Check.Out.Date...day.of.week, week.name)
}


















# Set 
country_freq <- data %>%
  group_by(country) %>%
  summarise(Count = n()) %>%
  mutate(Country_Grouped = if_else(Count < 2000, "Other", as.character(country)))

data <- data %>%
  left_join(country_freq, by = "country") %>%
  mutate(country = Country_Grouped) %>%
  select(-c(Count, Country_Grouped))

# adr > 0
data <- data[data$adr > 0,]

# children
data$children[is.na(data$children)] <- 0

# meal
data$meal[data$meal %in% c("Undefined", "FB")] <- "Other"

# market_segment
data$market_segment[data$market_segment %in% c("Aviation", "Complementary", "Undefined")] <- "Other" (consider keeping aviation)
→ # Remove rows where market_segment is "Undefined"
hotel.df <- hotel.df[hotel.df$market_segment != "Undefined", ]


# distribution_channel
data$distribution_channel[data$distribution_channel %in% c("GDS", "Undefined")] <- "Other"
→ # Remove rows where market_segment is "Undefined"
hotel.df <- hotel.df[hotel.df$distribution_channel != "Undefined", ]

# reserved_room_type
data$reserved_room_type[data$reserved_room_type %in% c("H", "I", "K", “P”, "L")] <- "Other"

# assigned_room_type
data$assigned_room_type[data$assigned_room_type %in% c("H", "I", "K", “P”, "L")] <- "Other"


# deposit_type
data$deposit_type[data$deposit_type %in% c("Non Refund", "Refundable")] <- "Deposit"

# customer_type
data$deposit_type[data$customer_type %in% c("Contract", "Group")] <- "Non-Transient"



